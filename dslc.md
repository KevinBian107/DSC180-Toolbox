# [Data Science Life Cycle](/README.md)

***Notes copied from dsc [180A capstone website](https://dsc-capstone.org/2025-26/lessons/03/)***

At a high level, the data science lifecycle looks a lot like the scientific method we saw in elementary school.

<p align="center">
  <img src="img/scientific-method.png" alt="scientific-method" width="40%">
</p>


While we may begin with a single question, after performing exploratory data analysis and building a model to answer our question, we will likely refine your original question or come up with more questions to investigate. As a result, the code that we write needs to be written so that it can support iteratively changing questions and analyses. Specifically, the code for a data science project needs to be:

1. **Flexibly written**: Caplable of adapting to changing questions or experimental configurations.
2. **Clearly documented**：Good writeup about what each piece does both to us and others using our code.
3. **Reproducible** Others should be able to run it themselves.

In theory, that sounds pretty straightforward. However, as we saw in DSC 80 and in practice, the real data science lifecycle is anything bu that. There are much parts to it, **the devils are hidden inside the details**:

<p align="center">
  <img src="img/DSLC.png" alt="DSLC" width="40%">
</p>

This makes it even more crucial that you follow the three principles outlined above. If you’re not careful, it’s easy to fall in the trap of writing poorly organized code with many hard-coded pieces. This results in:

- Being able to execute fewer iterations of your project, and as a result, making slower progress on your project.

- Being unsure of what your code is even doing, increasing the likelihood of making mistakes while iterating and making it unclear what your conclusions even are.

- Making it less likely that others will be able to use and replicate the results of your project, resulting in your project fading into obscurity.

The purpose of this section is to show the ways to adhere to the three principles mentioned above, as this will increase the chances of successfully executing a project and actually producing good works. Visit ["The Anatomy of a Data Science Project" section](https://dsc-capstone.org/2025-26/lessons/03/#the-anatomy-of-a-data-science-project) for more detailed explaination of how the DS life cycle is connected with the ocde we write.

## Managing Project Components
There are a plethora of tools used in industry for managing data science projects, we will explore a few popular tools that help solve core issues that will be relevant even decades in the future, once the current slate of tools is replaced. These core issues revolve around the fact that your project will be made up of **several components**. For instance, we may have separate components for:

- Ingesting and cleaning raw data
- Creating visualizations
- Training models

Issues that we need to be aware of are:

- **Communication**: How do these components all communicate with one another? What are the inputs and outputs of each component? It’s important to be clear about what these are up front, to avoid confusion later on.

- **Isolation**: Our code should be written in a way such that each component is as isolated as possible.
  - When we want to make changes to one component (say, loading in another column at the very start of your pipeline), we don’t have to make changes to all of your other components.
  - It would suck to have to buy a new dashboard for a car every time when getting a flat tire – the same principle applies here.

- **Parallelization and Scale**: It should be clear when each part of the project needs to be run, and whether different parts can be run in parallel. Similarly, it should be clear which components of the project will need to scale as the project grows in scope (if we collect 10x more data, at what stages will we need more compute resources?)
  - Use of configuration files!!! We can specify and track hypotheses and desired outputs. As such, when new questions arise, you won’t have to re-write the code; instead, we'll just run it with different configurations.

## Best Practices
This [repository](https://github.com/KevinBian107/dsc-project-templates), is adapted from the official data science project template created for HDSI DSC capstone, which contains several examples of this template in use, each stored in a different branch:

- `skeleton`: Skeleton code for a simple example.
- `titanic`: Titanic ML classifier, with how to deal with API tokens.
- `EDA`: A generic EDA that creates autogenerated reports via notebooks.
- `nn_regression`: Training neural network regressor, with local dataset and anaconda requirements setup.
- `think_stats`: Statistical analysis with notebook usage.

More detailed readings are available in the ["Best Practice"](https://dsc-capstone.org/2025-26/lessons/03/#best-practices) section of the DSC capstone reading.

### Modularzied Hierarchical Processing
One good practice for making reproduciable code is to split work flows to different levels of works. For example, for data preprocessing, we can create **ETL** (Extract, Transform, Load) that calls many lower level functionalities. Then a `run.py` or a `run.sh` may call this ETL processing. An example ETL script would look like the following (copied from [this repository](https://github.com/KevinBian107/dsc-project-templates/blob/nn_regression/src/etl.py)):

```python
import os
import pandas as pd


def get_data(indir, outdir):
    '''
    Reads the data by creating a symlink between the 
    location of the downloaded data and /data
    '''
    # first create the data directory
    directory = "data"
    parent_dir = "./"
    path = os.path.join(parent_dir, directory)

    os.mkdir(path)

    # create a convenient hierarchical structure of folders inside /data
    directory1 = "raw"
    directory2 = "temp"
    directory3 = "out"
    parent_dir = "./data/"
    
    os.mkdir(os.path.join(parent_dir, directory1))
    os.mkdir(os.path.join(parent_dir, directory2))
    os.mkdir(os.path.join(parent_dir, directory3))
    
    # create the symlink
    os.symlink(indir, outdir) 
        
    return pd.read_csv(outdir)
```

### Jupyter Notebook-Specific Guidelines
Notebooks are meant for **analysis** and **communication** only, not for storing source code. The majority of the notebooks should be made up of Markdown and visualization; there should be very little code, and most of the code there should consist of calls to the functions in your source code. Whenever we have written code that should be included in version control, move it to the source code files.